{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cardioid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########## import libraries ############\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import IPython; from IPython.display import display, HTML\n",
    "def dfPrint(df):\n",
    "    display(HTML(df.to_html()))\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (38932, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23231</th>\n",
       "      <td>id33557</td>\n",
       "      <td>My mom and I had a wonderful stay @ The Jane. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37847</th>\n",
       "      <td>id48173</td>\n",
       "      <td>This Holiday Inn's location is great for a vac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26845</th>\n",
       "      <td>id37171</td>\n",
       "      <td>I feel like I'm writing a lot of five star rev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8156</th>\n",
       "      <td>id18482</td>\n",
       "      <td>Great location on the beach. Good restaurants ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13962</th>\n",
       "      <td>id24288</td>\n",
       "      <td>I stayed at the Westin LAX many times.\\nIt's t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# import data ##############\n",
    "def import_data():\n",
    "    train_path = \"./data/nlp_trip_advisor/train.csv\"\n",
    "    data = pd.read_csv(train_path)\n",
    "    data = data[data.Is_Response.isnull() == False]\n",
    "#     data['Is_Response'] = data['Is_Response'].map(int)\n",
    "    data = data[data['Description'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    data = pd.get_dummies(data, columns=[\"Is_Response\"])\n",
    "    data.drop(['Browser_Used', 'Device_Used', 'Is_Response_not happy'], axis=1, inplace=True)\n",
    "    data.columns = ['User_ID', \"Description\", \"Sentiment\"]\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "dfPrint(data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ clean data ##############\n",
    "def clean_decription(line):\n",
    "    words = list(set(line.split()))\n",
    "    words = list(sorted([x.lower() for x in words]))\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "    words2 = words.copy()\n",
    "    for word in list(set(words2).intersection(stop_words)):\n",
    "        words.remove(word)\n",
    "#     out = ' '.join(words)\n",
    "    out = ' '.join(e for e in words if e.isalnum())\n",
    "    if out == \"\":\n",
    "        print(words)\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     out.translate(table, string.punctuation)\n",
    "    return out\n",
    "data[\"Description\"] = data[\"Description\"].map(clean_decription)\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "#     break\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23716                                    clean comfy great\n",
      "14285    ac acceptable bad bed bugs carpet cheaper chel...\n",
      "7634     also art asked baths bit booked booked certain...\n",
      "1030     across aka ally along also always amazing anni...\n",
      "38702    actually all along annoying appointed carting ...\n",
      "Name: Description, dtype: object\n",
      "39586\n"
     ]
    }
   ],
   "source": [
    "############### define documents ################\n",
    "docs = data[\"Description\"]\n",
    "############# define class labels ################\n",
    "labels = data[\"Sentiment\"]\n",
    "############## prepare tokenizer #################\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "########## integer encode the documents ##########\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "print(docs.sample(5))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[461, 99, 15, 19, 654, 34, 846, 312, 429, 836, 86, 3, 580, 5, 78, 1480, 2, 7], [891, 1442, 52, 1256, 87, 79, 4511, 3652, 1305, 741, 151, 50, 21, 665, 2967, 864, 95, 314, 400, 875, 288, 70, 1523, 56, 76, 29, 196, 306, 581, 581, 532, 402, 348, 742, 40, 748, 1161, 793, 634, 1427, 111, 68, 443, 5930, 1144, 1177, 10, 820, 231, 98, 690, 1428, 1603, 65, 3, 21333, 35, 293, 4, 1223, 5, 8, 128, 484, 2, 523, 8798, 181, 36, 16754, 935, 7], [22, 22, 366, 271, 4716, 247, 1940, 39, 1675, 73, 99, 83, 2705, 15, 1785, 108, 1124, 21, 47, 310, 26, 149, 489, 1451, 1086, 509, 23, 211, 16, 44, 42, 6, 233, 1, 1332, 349, 1327, 40, 57, 12, 193, 2527, 307, 7637, 536, 735, 1963, 49, 4882, 429, 1220, 55, 224, 14247, 571, 86, 1397, 3, 9, 121, 35, 4, 326, 2, 10627, 166, 118, 252, 18, 11, 7], [4961, 313, 235, 87, 73, 21, 1357, 289, 3081, 355, 28, 651, 6, 2002, 196, 209, 155, 596, 596, 715, 169, 3, 3, 17, 3289, 701, 8, 2102, 71, 41, 88, 11, 717], [7638, 319, 100, 823, 37, 699, 521, 226, 1043, 360, 2284, 1333, 1277, 379, 960, 132, 19, 7639, 654, 794, 289, 1560, 407, 204, 2185, 467, 43, 738, 278, 16, 1492, 3511, 1, 62, 292, 317, 209, 2780, 179, 162, 2616, 444, 144, 59, 80, 307, 1005, 111, 1398, 1373, 24, 2067, 67, 152, 55, 177, 705, 1258, 1872, 203, 629, 401, 48, 1964, 512, 448, 261, 335, 1301, 3, 9, 159, 672, 16755, 1419, 17, 140, 286, 1000, 253, 8, 78, 582, 1945, 2, 1811, 97, 180, 985, 1550, 537, 18, 11, 205, 38, 294, 903, 153, 186, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_docs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "of\n",
      "to\n",
      "and\n",
      "in\n",
      "a\n",
      "for\n",
      "that\n",
      "on\n",
      "is\n",
      "was\n",
      "with\n",
      "he\n",
      "as\n",
      "it\n",
      "by\n",
      "at\n",
      "from\n",
      "his\n",
      "an\n",
      "be\n",
      "has\n",
      "are\n",
      "have\n",
      "but\n",
      "were\n",
      "not\n",
      "this\n",
      "who\n",
      "they\n",
      "had\n",
      "i\n",
      "which\n",
      "will\n",
      "their\n",
      "or\n",
      "its\n",
      "after\n",
      "been\n",
      "we\n",
      "more\n",
      "about\n",
      "up\n",
      "when\n",
      "there\n",
      "all\n",
      "out\n",
      "she\n",
      "other\n",
      "her\n",
      "than\n",
      "over\n",
      "into\n",
      "some\n",
      "you\n",
      "if\n",
      "no\n",
      "can\n",
      "do\n",
      "only\n",
      "most\n",
      "against\n",
      "so\n",
      "them\n",
      "what\n",
      "him\n",
      "during\n",
      "before\n",
      "while\n",
      "where\n",
      "because\n",
      "now\n",
      "between\n",
      "did\n",
      "just\n",
      "under\n",
      "such\n",
      "then\n",
      "any\n",
      "through\n",
      "being\n",
      "down\n",
      "off\n",
      "both\n",
      "those\n",
      "these\n",
      "our\n",
      "here\n",
      "should\n",
      "very\n",
      "my\n",
      "how\n",
      "until\n",
      "same\n",
      "won\n",
      "each\n",
      "does\n",
      "own\n",
      "me\n",
      "few\n",
      "too\n",
      "again\n",
      "your\n",
      "once\n",
      "further\n",
      "having\n",
      "himself\n",
      "why\n",
      "am\n",
      "doing\n",
      "themselves\n",
      "itself\n",
      "above\n",
      "whom\n",
      "below\n",
      "s\n",
      "re\n",
      "d\n",
      "m\n",
      "nor\n",
      "t\n",
      "herself\n",
      "myself\n",
      "don\n",
      "y\n",
      "ma\n",
      "o\n",
      "yourself\n",
      "ourselves\n",
      "haven\n",
      "ours\n",
      "theirs\n",
      "shan\n",
      "yours\n",
      "hers\n",
      "ain\n",
      "ll\n",
      "yourselves\n",
      "ve\n",
      "doesn\n",
      "didn\n",
      "isn\n",
      "aren\n",
      "wasn\n",
      "couldn\n",
      "hasn\n",
      "shouldn\n",
      "wouldn\n",
      "weren\n",
      "Loaded 399851 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########### load the whole embedding into memory ############\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for num, line in enumerate(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in stop_words :\n",
    "        print (word)\n",
    "        continue\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "max_width = 100\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n"
     ]
    }
   ],
   "source": [
    "########### create a weight matrix for words in training docs ############\n",
    "embedding_matrix = zeros((vocab_size, max_width))\n",
    "mil=0\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        mil+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(mil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          3958600   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                500025    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 4,461,901\n",
      "Trainable params: 4,461,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "################ define model#################\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, max_width, weights=[embedding_matrix], input_length=max_length, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "############## compile the model ##############\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc','binary_accuracy'])\n",
    "############# summarize the model ##############\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24916 samples, validate on 6229 samples\n",
      "Epoch 1/3\n",
      "24916/24916 [==============================] - 47s 2ms/step - loss: 0.5044 - acc: 0.7676 - binary_accuracy: 0.7676 - val_loss: 0.4124 - val_acc: 0.8367 - val_binary_accuracy: 0.8367\n",
      "Epoch 2/3\n",
      "24916/24916 [==============================] - 46s 2ms/step - loss: 0.3566 - acc: 0.8698 - binary_accuracy: 0.8698 - val_loss: 0.4035 - val_acc: 0.8422 - val_binary_accuracy: 0.8422\n",
      "Epoch 3/3\n",
      "24916/24916 [==============================] - 50s 2ms/step - loss: 0.2615 - acc: 0.9115 - binary_accuracy: 0.9115 - val_loss: 0.4346 - val_acc: 0.8395 - val_binary_accuracy: 0.8395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7487d0d8d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### split into train test data #################\n",
    "padded_docs_train, padded_docs_test, labels_train, labels_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=69)\n",
    "################### fit the model ##########3#############\n",
    "model.fit(padded_docs_train, labels_train, epochs = 3, validation_split = 0.2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7787/7787 [==============================] - 1s 135us/step\n",
      "Accuracy: 83.986131\n",
      "binaryAccuracy: 83.986131\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy,binaryAccuracy = model.evaluate(padded_docs_test, labels_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('binaryAccuracy: %f' % (binaryAccuracy*100))\n",
    "predicted = model.predict(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
