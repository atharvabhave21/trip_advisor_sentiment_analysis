{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/atharv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import IPython; from IPython.display import display, HTML\n",
    "def dfPrint(df):\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (38932, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18929</th>\n",
       "      <td>id29255</td>\n",
       "      <td>I don't know what my firm paid for this one. B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36974</th>\n",
       "      <td>id47300</td>\n",
       "      <td>I'm not sure where the SLS brand is trying to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>id20927</td>\n",
       "      <td>My wife and I stayed at the U.S Grant mid augu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13403</th>\n",
       "      <td>id23729</td>\n",
       "      <td>Quaint hotel. Nice and clean room. Room amenit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23116</th>\n",
       "      <td>id33442</td>\n",
       "      <td>We spent a week at the River Inn and thoroughl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24679</th>\n",
       "      <td>id35005</td>\n",
       "      <td>The view of marine bay and the beach is nice a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>id13762</td>\n",
       "      <td>We stayed here for a short trip to Chicago. Th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9227</th>\n",
       "      <td>id19553</td>\n",
       "      <td>We just stayed at this hotel during a one week...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>id22326</td>\n",
       "      <td>Excellent choice for business traveler or tour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7246</th>\n",
       "      <td>id17572</td>\n",
       "      <td>We stayed over the Independence weekend for a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>id15604</td>\n",
       "      <td>We were in town for a conference and were put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>id12340</td>\n",
       "      <td>After staying in Vegas and Anaheim we were loo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9948</th>\n",
       "      <td>id20274</td>\n",
       "      <td>Overall our stay was nice here. The hotel is o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7667</th>\n",
       "      <td>id17993</td>\n",
       "      <td>I have stayed in many hotels while traveling t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10365</th>\n",
       "      <td>id20691</td>\n",
       "      <td>I want to preface this review by pointing out ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def import_data():\n",
    "    train_path = \"./data/nlp_trip_advisor/train.csv\"\n",
    "    data = pd.read_csv(train_path)\n",
    "    data = data[data.Is_Response.isnull() == False]\n",
    "#     data['Is_Response'] = data['Is_Response'].map(int)\n",
    "    data = data[data['Description'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    data = pd.get_dummies(data, columns=[\"Is_Response\"])\n",
    "    data.drop(['Browser_Used', 'Device_Used', 'Is_Response_not happy'], axis=1, inplace=True)\n",
    "    data.columns = ['User_ID', \"Description\", \"Sentiment\"]\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "data = data.iloc[:]\n",
    "data.to_csv(\"./data/data_train.csv\")\n",
    "dfPrint(data.sample(5))\n",
    "# data['Sentiment'].value_counts()\n",
    "data_sent_1 = data[data[\"Sentiment\"]==1]\n",
    "data_sent_1.to_csv(\"./data/data_sent_1_train.csv\")\n",
    "dfPrint(data_sent_1.sample(5))\n",
    "data_sent_0 = data[data[\"Sentiment\"]==0]\n",
    "data_sent_0.to_csv(\"./data/data_sent_0_train.csv\")\n",
    "dfPrint(data_sent_0.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_decription(line):\n",
    "    words = list(set(line.split()))\n",
    "    words = list(sorted([x.lower() for x in words]))\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "    words2 = words.copy()\n",
    "    for word in list(set(words2).intersection(stop_words)):\n",
    "        words.remove(word)\n",
    "#     out = ' '.join(words)\n",
    "    out = ' '.join(e for e in words if e.isalnum())\n",
    "    if out == \"\":\n",
    "        print(words)\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     out.translate(table, string.punctuation)\n",
    "    return out\n",
    "data[\"Description\"] = data[\"Description\"].map(clean_decription)\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "#     break\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38932\n",
      "38932\n",
      "38932\n",
      "38932\n"
     ]
    }
   ],
   "source": [
    "docs = data[\"Description\"]; labels = data[\"Sentiment\"]\n",
    "print(len (data[\"Sentiment\"]))\n",
    "print(len (labels))\n",
    "print(len (data[\"Description\"]))\n",
    "print(len(docs))\n",
    "docs_sent_1 = data_sent_1[\"Description\"]; labels_sent_1 = data_sent_1[\"Sentiment\"]\n",
    "docs_sent_0 = data_sent_0[\"Description\"]; labels_sent_0 = data_sent_0[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39586\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39585\n",
      "39585\n"
     ]
    }
   ],
   "source": [
    "print(len(t.word_counts))\n",
    "# print(t.word_counts)\n",
    "# wordCounts = sorted(t.word_counts.iteritems(), key=lambda k: (v,k))\n",
    "wordCounts = sorted(t.word_counts.items(), key=lambda x:x[1], reverse=True)\n",
    "print(len(wordCounts))\n",
    "# print(wordCounts.items()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf = t.texts_to_matrix(docs, mode='count')\n",
    "\n",
    "# fileObject = open(\"./data/PickledForKhadtare\",'r')  \n",
    "# # load the object from the file into var b\n",
    "# # fileObject.encode('utf-8').strip()\n",
    "# tf_idf = pickle.load(pybytes(readbytes(fileObject)))\n",
    "# fileObject.close()\n",
    "\n",
    "# print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38932\n",
      "[[ 461   99   15 ...    0    0    0]\n",
      " [ 891 1442   52 ...    0    0    0]\n",
      " [  22   22  366 ...    0    0    0]\n",
      " ...\n",
      " [1052   22  100 ...    0    0    0]\n",
      " [1010   99 4366 ...    0    0    0]\n",
      " [  87 6446 7876 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 250\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(len(padded_docs))\n",
    "print(padded_docs)\n",
    "# load the whole embedding into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 399851 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('./data/glove/glove.6B.100d.txt')\n",
    "vocab_size = 0\n",
    "maxLen = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "#     print (word, end = '\\t')\n",
    "    word = values[0]\n",
    "    maxLen = max(maxLen, len(word))\n",
    "    if word in stop_words:\n",
    "#         print (word)\n",
    "        continue\n",
    "    vocab_size += 1\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "mil = 0\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        mil+=1\n",
    "#         print (word, end = '\\t')\n",
    "print(mil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(75, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(25, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 100)          39985100  \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 25000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 75)                1875075   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 76        \n",
      "=================================================================\n",
      "Total params: 41,860,251\n",
      "Trainable params: 41,860,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "38932\n",
      "Train on 31145 samples, validate on 7787 samples\n",
      "Epoch 1/50\n",
      "21248/31145 [===================>..........] - ETA: 3:50 - loss: 0.5072 - acc: 0.7592"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "print (len(labels))\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=1, validation_split=0.2)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
