{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cardioid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########## import libraries ############\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import IPython; from IPython.display import display, HTML\n",
    "def dfPrint(df):\n",
    "    display(HTML(df.to_html()))\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (38932, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11265</th>\n",
       "      <td>id21591</td>\n",
       "      <td>I stayed at this hotel on a Friday and Saturda...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>id19405</td>\n",
       "      <td>Beautiful hotel directly on the harbor. Checke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16965</th>\n",
       "      <td>id27291</td>\n",
       "      <td>Enjoyable stay in SF. Hyatt was lovely and the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24743</th>\n",
       "      <td>id35069</td>\n",
       "      <td>This was my first time staying in a Staybridge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11374</th>\n",
       "      <td>id21700</td>\n",
       "      <td>I just stayed - nights at the Parc --. Great l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# import data ##############\n",
    "def import_data():\n",
    "    train_path = \"./data/nlp_trip_advisor/train.csv\"\n",
    "    data = pd.read_csv(train_path)\n",
    "    data = data[data.Is_Response.isnull() == False]\n",
    "#     data['Is_Response'] = data['Is_Response'].map(int)\n",
    "    data = data[data['Description'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    data = pd.get_dummies(data, columns=[\"Is_Response\"])\n",
    "    data.drop(['Browser_Used', 'Device_Used', 'Is_Response_not happy'], axis=1, inplace=True)\n",
    "    data.columns = ['User_ID', \"Description\", \"Sentiment\"]\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "dfPrint(data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ clean data ##############\n",
    "def clean_decription(line):\n",
    "    words = list(set(line.split()))\n",
    "    words = list(sorted([x.lower() for x in words]))\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "    words2 = words.copy()\n",
    "    for word in list(set(words2).intersection(stop_words)):\n",
    "        words.remove(word)\n",
    "#     out = ' '.join(words)\n",
    "    out = ' '.join(e for e in words if e.isalnum())\n",
    "    if out == \"\":\n",
    "        print(words)\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     out.translate(table, string.punctuation)\n",
    "    return out\n",
    "data[\"Description\"] = data[\"Description\"].map(clean_decription)\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "#     break\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### define documents ################\n",
    "docs = data[\"Description\"]\n",
    "############# define class labels ################\n",
    "labels = data[\"Sentiment\"]\n",
    "############## prepare tokenizer #################\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "########## integer encode the documents ##########\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "of\n",
      "to\n",
      "and\n",
      "in\n",
      "a\n",
      "for\n",
      "that\n",
      "on\n",
      "is\n",
      "was\n",
      "with\n",
      "he\n",
      "as\n",
      "it\n",
      "by\n",
      "at\n",
      "from\n",
      "his\n",
      "an\n",
      "be\n",
      "has\n",
      "are\n",
      "have\n",
      "but\n",
      "were\n",
      "not\n",
      "this\n",
      "who\n",
      "they\n",
      "had\n",
      "i\n",
      "which\n",
      "will\n",
      "their\n",
      "or\n",
      "its\n",
      "after\n",
      "been\n",
      "we\n",
      "more\n",
      "about\n",
      "up\n",
      "when\n",
      "there\n",
      "all\n",
      "out\n",
      "she\n",
      "other\n",
      "her\n",
      "than\n",
      "over\n",
      "into\n",
      "some\n",
      "you\n",
      "if\n",
      "no\n",
      "can\n",
      "do\n",
      "only\n",
      "most\n",
      "against\n",
      "so\n",
      "them\n",
      "what\n",
      "him\n",
      "during\n",
      "before\n",
      "while\n",
      "where\n",
      "because\n",
      "now\n",
      "between\n",
      "did\n",
      "just\n",
      "under\n",
      "such\n",
      "then\n",
      "any\n",
      "through\n",
      "being\n",
      "down\n",
      "off\n",
      "both\n",
      "those\n",
      "these\n",
      "our\n",
      "here\n",
      "should\n",
      "very\n",
      "my\n",
      "how\n",
      "until\n",
      "same\n",
      "won\n",
      "each\n",
      "does\n",
      "own\n",
      "me\n",
      "few\n",
      "too\n",
      "again\n",
      "your\n",
      "once\n",
      "further\n",
      "having\n",
      "himself\n",
      "why\n",
      "am\n",
      "doing\n",
      "themselves\n",
      "itself\n",
      "above\n",
      "whom\n",
      "below\n",
      "s\n",
      "re\n",
      "d\n",
      "m\n",
      "nor\n",
      "t\n",
      "herself\n",
      "myself\n",
      "don\n",
      "y\n",
      "ma\n",
      "o\n",
      "yourself\n",
      "ourselves\n",
      "haven\n",
      "ours\n",
      "theirs\n",
      "shan\n",
      "yours\n",
      "hers\n",
      "ain\n",
      "ll\n",
      "yourselves\n",
      "ve\n",
      "doesn\n",
      "didn\n",
      "isn\n",
      "aren\n",
      "wasn\n",
      "couldn\n",
      "hasn\n",
      "shouldn\n",
      "wouldn\n",
      "weren\n",
      "Loaded 399851 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########### load the whole embedding into memory ############\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for num, line in enumerate(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in stop_words :\n",
    "        print (word)\n",
    "        continue\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n"
     ]
    }
   ],
   "source": [
    "########### create a weight matrix for words in training docs ############\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "mil=0\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        mil+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(mil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          3958600   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25)                12600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 3,974,181\n",
      "Trainable params: 3,974,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "################ define model#################\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=True)\n",
    "model.add(e)\n",
    "model.add(LSTM(25))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(50,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(15,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "############## compile the model ##############\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc','binary_accuracy'])\n",
    "############# summarize the model ##############\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24916 samples, validate on 6229 samples\n",
      "Epoch 1/3\n",
      "24916/24916 [==============================] - 358s 14ms/step - loss: 0.6345 - acc: 0.6789 - binary_accuracy: 0.6789 - val_loss: 0.6268 - val_acc: 0.6808 - val_binary_accuracy: 0.6808\n",
      "Epoch 2/3\n",
      "24916/24916 [==============================] - 366s 15ms/step - loss: 0.6298 - acc: 0.6793 - binary_accuracy: 0.6793 - val_loss: 0.6260 - val_acc: 0.6808 - val_binary_accuracy: 0.6808\n",
      "Epoch 3/3\n",
      "24916/24916 [==============================] - 368s 15ms/step - loss: 0.6288 - acc: 0.6793 - binary_accuracy: 0.6793 - val_loss: 0.6254 - val_acc: 0.6808 - val_binary_accuracy: 0.6808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb517fdf860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### split into train test data #################\n",
    "padded_docs_train, padded_docs_test, labels_train, labels_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=69)\n",
    "################### fit the model ##########3#############\n",
    "model.fit(padded_docs_train, labels_train, epochs=3, validation_split = 0.2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7787/7787 [==============================] - 24s 3ms/step\n",
      "Accuracy: 68.755618\n",
      "binaryAccuracy: 68.755618\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy,binaryAccuracy = model.evaluate(padded_docs_test, labels_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('binaryAccuracy: %f' % (binaryAccuracy*100))\n",
    "predicted = model.predict(padded_docs_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
