{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cardioid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "########## import libraries ############\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import IPython; from IPython.display import display, HTML\n",
    "def dfPrint(df):\n",
    "    display(HTML(df.to_html()))\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (38932, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32033</th>\n",
       "      <td>id42359</td>\n",
       "      <td>We stayed at the hotel for a mini-conference. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26516</th>\n",
       "      <td>id36842</td>\n",
       "      <td>I decided at the last minute to drive up from ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31290</th>\n",
       "      <td>id41616</td>\n",
       "      <td>I stayed at this hotel recently on my way to &amp;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>id12368</td>\n",
       "      <td>Yes, it's an older hotel and may not be the fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29938</th>\n",
       "      <td>id40264</td>\n",
       "      <td>We booked our trip through Dreamticket but we ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############# import data ##############\n",
    "def import_data():\n",
    "    train_path = \"./data/nlp_trip_advisor/train.csv\"\n",
    "    data = pd.read_csv(train_path)\n",
    "    data = data[data.Is_Response.isnull() == False]\n",
    "#     data['Is_Response'] = data['Is_Response'].map(int)\n",
    "    data = data[data['Description'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    data = pd.get_dummies(data, columns=[\"Is_Response\"])\n",
    "    data.drop(['Browser_Used', 'Device_Used', 'Is_Response_not happy'], axis=1, inplace=True)\n",
    "    data.columns = ['User_ID', \"Description\", \"Sentiment\"]\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "dfPrint(data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ clean data ##############\n",
    "def clean_decription(line):\n",
    "    words = list(set(line.split()))\n",
    "    words = list(sorted([x.lower() for x in words]))\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "    words2 = words.copy()\n",
    "    for word in list(set(words2).intersection(stop_words)):\n",
    "        words.remove(word)\n",
    "#     out = ' '.join(words)\n",
    "    out = ' '.join(e for e in words if e.isalnum())\n",
    "    if out == \"\":\n",
    "        print(words)\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     out.translate(table, string.punctuation)\n",
    "    return out\n",
    "data[\"Description\"] = data[\"Description\"].map(clean_decription)\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "#     break\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[461, 99, 15, 19, 654, 34, 846, 312, 429, 836, 86, 3, 580, 5, 78, 1480, 2, 7], [891, 1442, 52, 1256, 87, 79, 4511, 3652, 1305, 741, 151, 50, 21, 665, 2967, 864, 95, 314, 400, 875, 288, 70, 1523, 56, 76, 29, 196, 306, 581, 581, 532, 402, 348, 742, 40, 748, 1161, 793, 634, 1427, 111, 68, 443, 5930, 1144, 1177, 10, 820, 231, 98, 690, 1428, 1603, 65, 3, 21333, 35, 293, 4, 1223, 5, 8, 128, 484, 2, 523, 8798, 181, 36, 16754, 935, 7], [22, 22, 366, 271, 4716, 247, 1940, 39, 1675, 73, 99, 83, 2705, 15, 1785, 108, 1124, 21, 47, 310, 26, 149, 489, 1451, 1086, 509, 23, 211, 16, 44, 42, 6, 233, 1, 1332, 349, 1327, 40, 57, 12, 193, 2527, 307, 7637, 536, 735, 1963, 49, 4882, 429, 1220, 55, 224, 14247, 571, 86, 1397, 3, 9, 121, 35, 4, 326, 2, 10627, 166, 118, 252, 18, 11, 7], [4961, 313, 235, 87, 73, 21, 1357, 289, 3081, 355, 28, 651, 6, 2002, 196, 209, 155, 596, 596, 715, 169, 3, 3, 17, 3289, 701, 8, 2102, 71, 41, 88, 11, 717], [7638, 319, 100, 823, 37, 699, 521, 226, 1043, 360, 2284, 1333, 1277, 379, 960, 132, 19, 7639, 654, 794, 289, 1560, 407, 204, 2185, 467, 43, 738, 278, 16, 1492, 3511, 1, 62, 292, 317, 209, 2780, 179, 162, 2616, 444, 144, 59, 80, 307, 1005, 111, 1398, 1373, 24, 2067, 67, 152, 55, 177, 705, 1258, 1872, 203, 629, 401, 48, 1964, 512, 448, 261, 335, 1301, 3, 9, 159, 672, 16755, 1419, 17, 140, 286, 1000, 253, 8, 78, 582, 1945, 2, 1811, 97, 180, 985, 1550, 537, 18, 11, 205, 38, 294, 903, 153, 186, 7], [32, 104, 15, 9303, 19, 856, 478, 4883, 589, 2670, 217, 13, 6, 6, 1240, 12, 49, 93, 176, 14, 109, 10, 6082, 27, 325, 6083, 9, 3224, 5, 9894, 854, 2, 181, 865, 2257, 777, 502, 625, 11, 594, 358, 7, 8799], [2685, 39, 15, 515, 19, 453, 415, 799, 6847, 1, 62, 209, 135, 349, 348, 285, 1097, 49, 397, 5287, 4, 5, 8, 136, 386, 477, 41, 72], [54, 763, 872, 79, 741, 110, 5158, 556, 157, 1782, 69, 1091, 276, 75, 23, 1492, 4452, 1015, 1, 40, 12, 5635, 4453, 971, 2068, 93, 14, 713, 30, 231, 743, 259, 839, 2258, 390, 48, 2633, 65, 3, 857, 1039, 5408, 4, 8, 341, 3168, 180, 309, 153, 7], [1186, 2285, 139, 1333, 2583, 1233, 21, 63, 864, 794, 1034, 289, 491, 2706, 375, 163, 1707, 43, 470, 559, 1, 583, 3476, 81, 1178, 1101, 2720, 111, 24, 496, 3602, 843, 48, 3, 9, 124, 14248, 8, 78, 2, 6425, 103, 860, 153], [269, 191, 15, 108, 724, 149, 34, 6626, 13, 6, 560, 196, 1, 357, 850, 146, 86, 14249, 2019, 334, 2, 61, 605, 153]]\n"
     ]
    }
   ],
   "source": [
    "############### define documents ################\n",
    "docs = data[\"Description\"]\n",
    "############# define class labels ################\n",
    "labels = data[\"Sentiment\"]\n",
    "############## prepare tokenizer #################\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "########## integer encode the documents ##########\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "of\n",
      "to\n",
      "and\n",
      "in\n",
      "a\n",
      "for\n",
      "that\n",
      "on\n",
      "is\n",
      "was\n",
      "with\n",
      "he\n",
      "as\n",
      "it\n",
      "by\n",
      "at\n",
      "from\n",
      "his\n",
      "an\n",
      "be\n",
      "has\n",
      "are\n",
      "have\n",
      "but\n",
      "were\n",
      "not\n",
      "this\n",
      "who\n",
      "they\n",
      "had\n",
      "i\n",
      "which\n",
      "will\n",
      "their\n",
      "or\n",
      "its\n",
      "after\n",
      "been\n",
      "we\n",
      "more\n",
      "about\n",
      "up\n",
      "when\n",
      "there\n",
      "all\n",
      "out\n",
      "she\n",
      "other\n",
      "her\n",
      "than\n",
      "over\n",
      "into\n",
      "some\n",
      "you\n",
      "if\n",
      "no\n",
      "can\n",
      "do\n",
      "only\n",
      "most\n",
      "against\n",
      "so\n",
      "them\n",
      "what\n",
      "him\n",
      "during\n",
      "before\n",
      "while\n",
      "where\n",
      "because\n",
      "now\n",
      "between\n",
      "did\n",
      "just\n",
      "under\n",
      "such\n",
      "then\n",
      "any\n",
      "through\n",
      "being\n",
      "down\n",
      "off\n",
      "both\n",
      "those\n",
      "these\n",
      "our\n",
      "here\n",
      "should\n",
      "very\n",
      "my\n",
      "how\n",
      "until\n",
      "same\n",
      "won\n",
      "each\n",
      "does\n",
      "own\n",
      "me\n",
      "few\n",
      "too\n",
      "again\n",
      "your\n",
      "once\n",
      "further\n",
      "having\n",
      "himself\n",
      "why\n",
      "am\n",
      "doing\n",
      "themselves\n",
      "itself\n",
      "above\n",
      "whom\n",
      "below\n",
      "s\n",
      "re\n",
      "d\n",
      "m\n",
      "nor\n",
      "t\n",
      "herself\n",
      "myself\n",
      "don\n",
      "y\n",
      "ma\n",
      "o\n",
      "yourself\n",
      "ourselves\n",
      "haven\n",
      "ours\n",
      "theirs\n",
      "shan\n",
      "yours\n",
      "hers\n",
      "ain\n",
      "ll\n",
      "yourselves\n",
      "ve\n",
      "doesn\n",
      "didn\n",
      "isn\n",
      "aren\n",
      "wasn\n",
      "couldn\n",
      "hasn\n",
      "shouldn\n",
      "wouldn\n",
      "weren\n",
      "Loaded 399851 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########### load the whole embedding into memory ############\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for num, line in enumerate(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in stop_words :\n",
    "        print (word)\n",
    "        continue\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n"
     ]
    }
   ],
   "source": [
    "########### create a weight matrix for words in training docs ############\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "mil=0\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        mil+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(mil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          3958600   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 166, 15)           52515     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2490)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2491      \n",
      "=================================================================\n",
      "Total params: 4,013,606\n",
      "Trainable params: 4,013,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "################ define model#################\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=True )\n",
    "model.add(e)\n",
    "model.add(Conv1D(filters= 15 ,kernel_size= 35 ,activation=\"relu\",strides=1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "############## compile the model ##############\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc','binary_accuracy'])\n",
    "############# summarize the model ##############\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24916 samples, validate on 6229 samples\n",
      "Epoch 1/3\n",
      " 6144/24916 [======>.......................] - ETA: 1:26 - loss: 0.5121 - acc: 0.7458 - binary_accuracy: 0.7458"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1ca026b3dd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpadded_docs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_docs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m69\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m################### fit the model #####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_docs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############## split into train test data ##############\n",
    "padded_docs_train, padded_docs_test, labels_train, labels_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=69)\n",
    "################### fit the model #####################\n",
    "model.fit(padded_docs_train, labels_train, epochs=3, validation_split = 0.2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy,binaryAccuracy = model.evaluate(padded_docs_test, labels_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('binaryAccuracy: %f' % (binaryAccuracy*100))\n",
    "predicted = model.predict(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
